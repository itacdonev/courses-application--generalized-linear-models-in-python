---
title: Insert title here
key: 65f818a323555163f8ee44d12e7c3566

---
## Why Generalized Linear Models?

```yaml
type: "TitleSlide"
key: "d7295e1d38"
```

`@lower_third`

name: Ita Cirovic Donev
title: 


`@script`
> Hi, my name is Ita and I welcome you to this course on Generalized Linear Models in Python. Generalized linear models or GLMs for short provide a versatile framework for statistical modeling of data and are used often to solve real world problems. We will examine several of such problems in lessons to come. So stay tuned!


---
## Introduction

```yaml
type: "FullSlide"
key: "284a1d46df"
```

`@part1`
- Motivation for GLM {{1}}
- Building blocks of GLM {{2}}
- Build knowledge on practical examples using Poisson and logistic regression{{3}}

Objectives: {{4}}
- understand how GLM works {{5}}
- formulate and implement a GLM in Python {{6}} 
- interpret the model results {{7}}
- make prediction {{8}}


`@script`
- We will begin the course by motivating the need for GLMs with the review of linear models and how we arrive at the generalized term in the GLM name. 

- Furthermore, we will define the components of the GLM and how to implement them in Python. 
For the rest of the course we will focus on learning the statistical aspects of fitting the model, understanding and analyzing the output and making predictions, by examining various data sets. For this we will use Poisson and logistic regression. By the end of the course you will have both a theoretical understanding and the working knowledge of how to implement a GLM in practice. 

- So to sum up, our main objectives for the course are to 
- understand how GLM  works 
- be able to formulate and implemented a GLM in Python, 
- interpret the model results and finally, 
- learn how to use fitted model to make predictions on new data.


---
## Review of Linear Models

```yaml
type: "FullSlide"
key: "7b85aab958"
```

`@part1`
**Simple linear model**  {{0}}

$\normalsize{y = \beta_0 + \beta_1x_1 + \epsilon}$ {{0}}


`@script`



---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "69bb78300c"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{\color{red}y = \beta_0 + \beta_1x_1 + \epsilon}$


`@part2`
$y$ - response variable


`@script`



---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "0b1e4f0dbf"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \beta_0 + \beta_1\color{red}{x_1} + \epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable


`@script`



---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "5587486595"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \color{red}{\beta_0} + \color{red}{\beta_1}x_1 + \epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable  
$\beta$ - model parameters  
$\beta_0$ - intercept  
$\beta_1$ - slope


`@script`



---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "73897a6a17"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \beta_0 + \beta_1x_1 + \color{red}\epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable  
$\beta$ - model parameters  
$\beta_0$ - intercept  
$\beta_1$ - slope  
$\epsilon$ - random error


`@script`



---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "8ce5763dc6"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \beta_0 + \beta_1x_1 + \epsilon}$ 

&nbsp;

**General linear model**  
(classical GLM)  

$\normalsize{y = \beta_0 + \beta_1x_1 + ... + \beta_px_p + \epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable  
$\beta$ - model parameters  
$\beta_0$ - intercept  
$\beta_1$ - slope  
$\epsilon$ - random error


`@script`



---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "bc24f029fc"
```

`@part1`
Want to model

$\normalsize{E[y|X] = \mu}$

$\normalsize{ \mu = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$


`@part2`



`@script`
> One of the main goals of regression is to predict Y from the explanatory variables using the relationship as the following. Or in other words we model the information of how much the response variable changes on average for a unit increase in the explanatory variable.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "b9da57178e"
disable_transition: true
```

`@part1`
Want to model

$\normalsize{E[\color{blue}{y|X}] = \mu}$

$\normalsize{ \mu = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$


`@part2`



`@script`
> Note that we write the expectation as y conditional on x due to the fact that the inferences about the relationship of y and x assume knowledge of x.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "cbed00212c"
```

`@part1`
Want to model

$\normalsize{E[y|X] = \mu}$ 

$\normalsize{ \mu = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$


`@part2`
**Assumptions** 
- linear in parameters {{2}}
- errors: independent and normally distributed {{3}}
- constant variance for all $i$; $Var(y_i) = \sigma^2$ {{4}}


`@script`



---
## Review of Linear Models

```yaml
type: "FullSlide"
key: "115cd470fb"
```

`@part1`
**WHAT IF ... ?**  
- our measurements of the response are not continuous, but for example binary or count {{1}}
- the variance of $y$ depends on the mean {{2}}


`@script`
We will see in later videos how GLMs solve these problems. For now, let's motivate ourselves further by looking at an example.


---
## Motivating Example - Data

```yaml
type: "FullSlide"
key: "55ff39cc92"
```

`@part1`
- Contamination of ground water with arsenic in Bangladesh
- 3020 observations (`1`: 1737, `0`: 1283) 
- model the decision on switching wells 

|Variables|Name|Description|
|:-|-|:-|
|Response|`switch`|1 if change of water source occurred; 0 otherwise|
|Explanatory|`arsenic`| The level of arsenic contamination in the well|
|Explanatory|`distance`| Distance to the closest known safe well|
|Explanatory|`education`|Years of education of the head of the household|


`@script`
- model the decision on switching wells, i.e. changing the


---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "c497ddb85d"
```

`@part1`
```python
formula = 'switch ~ arsenic' 
``` {{1}}
```python
family = sm.families.Gaussian() 
``` {{3}}
```py
smf.glm(formula = formula, data = wells, family = family).fit()
``` {{4}}

![](http://assets.datacamp.com/production/repositories/3864/datasets/3ef6d3d112099677e1ca9fb09c9961cf2b1de7c9/Ch1_L1_Scatter_plot_arsenic.png)
{{2}}


`@script`



---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "551e61472a"
disable_transition: true
```

`@part1`
```python
formula = 'switch ~ arsenic' 

family = sm.families.Gaussian() 

smf.glm(formula = formula, data = wells, family = family).fit()
```
![](http://assets.datacamp.com/production/repositories/3864/datasets/de114467b003fea04daa83aae23dce73b4d1bade/Ch1_L1_Scatter_plot_LM_arsenic_basic.png)


`@script`



---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "9a7702ce6a"
disable_transition: true
```

`@part1`
```python
formula = 'switch ~ arsenic'

family = sm.families.Gaussian()

smf.glm(formula = formula, data = wells, family = family).fit()
``` 

![](http://assets.datacamp.com/production/repositories/3864/datasets/4ee031be36efd84878e82ca015eaf4d2160217ce/Ch1_L1_Scatter_plot_LM_arsenic.png)


`@script`



---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "4cc5f907b4"
```

`@part1`
Conclusion: 
- normality violation: distribution  of $y$ is highly skewed when $p$ is close to 0 or 1 
- variance of $y$ depends on the mean value 
- violates restriction that $0 \le p \le 1$


`@script`



---
## Motivating example - A correct model (GLM)

```yaml
type: "FullSlide"
key: "387f3c5591"
```

`@part1`
```python
formula = 'switch ~ arsenic' 
``` {{1}}
```python
family = sm.families.Binomial() 
``` {{2}}
```py
smf.glm(formula = formula, data = wells, family = family).fit()
``` {{3}}

![](http://assets.datacamp.com/production/repositories/3864/datasets/3ef6d3d112099677e1ca9fb09c9961cf2b1de7c9/Ch1_L1_Scatter_plot_arsenic.png)
{{1}}


`@script`



---
## Motivating example - A correct model (GLM)

```yaml
type: "FullSlide"
key: "33c3df5544"
disable_transition: true
```

`@part1`
```python
formula = 'switch ~ arsenic' 

family = sm.families.Binomial() 

smf.glm(formula = formula, data = wells, family = family).fit()
``` 

![](http://assets.datacamp.com/production/repositories/3864/datasets/846288f2ee857a9bdf6681e001d701ce6981487b/Ch1_L1_Scatter_plot_GLM_arsenic.png)


`@script`



---
## Motivating example - A correct model (GLM)

```yaml
type: "FullSlide"
key: "392587dd68"
```

`@part1`
Conclusion:
- Correct model formulation {{1}}
- predictions are bounded by (0,1) {{2}}
- take into account how the response variable is measured {{3}}


`@script`



---
## Insert title here...

```yaml
type: "FullSlide"
key: "b54b42e72a"
```

`@part1`
so what happened....let's take a peek of the GLM structure and connect LM and GLM and see the difference....this will be explained in the next video.


`@script`



---
## Types of Response Variables

```yaml
type: "FullSlide"
key: "f3ef0a4fe2"
```

`@part1`
- explanatory variables: categorical, continuous {{1}}
- responses are independent random variables {{2}}

| Response    	| Example                                     	| Method                           	|
|-------------	|:-------------------------------------------:	|:---------------------------------:|
| Continuous  	| `sale price`                                  | Multiple regression             	| 
{{3}}


`@script`



---
## Types of Response Variables

```yaml
type: "FullSlide"
key: "0aa0fd5995"
disable_transition: true
```

`@part1`
- explanatory variables: categorical, continuous
- responses are independent random variables

| Response    	| Example                                     	| Method                           	|
|-------------	|:-------------------------------------------:	|:---------------------------------:|
| Continuous  	| `sale price`                                  | Multiple regression             	| 
| Binary      	| `True, False`                         	    | Logistic regression              	|


`@script`



---
## Types of Response Variables

```yaml
type: "FullSlide"
key: "a876d49eb8"
disable_transition: true
```

`@part1`
- explanatory variables: categorical, continuous
- responses are independent random variables

| Response    	| Example                                     	| Method                           	|
|-------------	|:-------------------------------------------:	|:---------------------------------:|
| Continuous  	| `sale price`                                  | Multiple regression             	| 
| Binary      	| `True, False`                         	    | Logistic regression              	|
| Counts      	| `5, 7, 12, 4` votes                                 | Poisson regression               	|


`@script`



---
## Types of Response Variables

```yaml
type: "FullSlide"
key: "b139b03d68"
disable_transition: true
```

`@part1`
- explanatory variables: categorical, continuous
- responses are independent random variables

| Response    	| Example                                     	| Method                           	|
|-------------	|:-------------------------------------------:	|:---------------------------------:|
| Continuous  	| `sale price`                                  | Multiple regression             	| 
| Binary      	| `True, False`                         	    | Logistic regression              	|
| Counts      	| `5, 7, 12, 4` votes                                 | Poisson regression               	|
| Nominal 	    | `red, green, blue` 	                        | Multinominal logistic regression 	|


`@script`



---
## Types of Response Variables

```yaml
type: "FullSlide"
key: "54df08102b"
disable_transition: true
```

`@part1`
- explanatory variables: categorical, continuous
- responses are independent random variables

| Response    	| Example                                     	| Method                           	|
|-------------	|:-------------------------------------------:	|:---------------------------------:|
| Continuous  	| `sale price`                                  | Multiple regression             	| 
| Binary      	| `True, False`                         	    | Logistic regression              	|
| Counts      	| `5, 7, 12, 4` votes                                 | Poisson regression               	|
| Nominal 	    | `red, green, blue` 	                        | Multinominal logistic regression 	|
| Ordinal     	| `[1-10),[10-20),[>20)`                        | Ordinal logistic regression      	|


`@script`



---
## Types of Response Variables

```yaml
type: "FullSlide"
key: "ea23bd526b"
disable_transition: true
```

`@part1`
- explanatory variables: categorical, continuous
- responses are independent random variables

| Response    	| Example                                     	| Method                           	|
|-------------	|:-------------------------------------------:	|:---------------------------------:|
| Continuous  	| `sale price`                                  | Multiple regression             	| 
| $\color{blue}{\text{Binary}}$      	| `True, False`          | $\color{blue}{\text{Logistic Regression}}$              	|
| $\color{blue}{\text{Counts}}$      	| `5, 7, 12, 4` votes          | $\color{blue}{\text{Poisson regression}}$               	|
| Nominal 	    | `red, green, blue` 	                        | Multinominal logistic regression 	|
| Ordinal     	| `[1-10),[10-20),[>20)`                        | Ordinal logistic regression      	|


`@script`
Now that we have seen how the GLMs apply in situations where the main assumptions of the linear model are not satisfied, let's examine further in which situations we can apply the GLMs.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "72f692d178"
```

`@script`



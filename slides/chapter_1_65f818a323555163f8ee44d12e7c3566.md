---
title: Insert title here
key: 65f818a323555163f8ee44d12e7c3566

---
## Introduction to Generalized Linear Models

```yaml
type: "TitleSlide"
key: "d7295e1d38"
```

`@lower_third`

name: Ita Cirovic Donev
title: 


`@script`
> Hi, my name is Ita and I welcome you to this course on Generalized Linear Models in Python. Generalized linear models or GLMs for short provide a versatile framework for statistical modeling of data and are used often to solve real world problems. We will examine several of such problems in lessons to come. So stay tuned!


---
## Introduction

```yaml
type: "FullSlide"
key: "284a1d46df"
```

`@part1`
- Motivation for GLM {{1}}
- Building blocks of GLM {{2}}

Objectives: {{4}}
- understand how GLM works {{4}}
- formulate and implement a GLM in Python {{4}} 
- interpret the model results {{4}}
- make prediction {{4}}


`@script`
> We will begin the course by motivating the need for GLMs with the review of linear models and how we arrive at the generalized term in the GLM name. 
> Furthermore, we will define the components of the GLM and learn how to implement them in Python. 
For the rest of the course we will focus on learning the statistical aspects of fitting the model, understanding the output and making predictions. By the end of the course you will have both a theoretical understanding and the working knowledge of how to implement a GLM in practice


---
## Review of Linear Models

```yaml
type: "FullSlide"
key: "7b85aab958"
```

`@part1`
**Simple linear model**  {{0}}

$\normalsize{y = \beta_0 + \beta_1x_1 + \epsilon}$ {{0}}


`@script`
> In this first lesson we review the linear models and see how the generalized linear models extend this modeling framework, i.e. to variables that are not necessarily continuous. Recall that the simple linear model can be written as follows,


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "69bb78300c"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{\color{red}y = \beta_0 + \beta_1x_1 + \epsilon}$


`@part2`
$y$ - response variable


`@script`
> where y is the response variable, usually assumed to be normally distributed


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "0b1e4f0dbf"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \beta_0 + \beta_1\color{red}{x_1} + \epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable


`@script`
> x is the explanatory or predictor variable


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "5587486595"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \color{red}{\beta_0} + \color{red}{\beta_1}x_1 + \epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable  
$\beta$ - model parameters  
$\beta_0$ - intercept  
$\beta_1$ - slope


`@script`
> betas are fixed and unknown parameters that we estimate by fitting the model, where Beta_0 denotes the intercept  and beta_1 is the slope.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "73897a6a17"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \beta_0 + \beta_1x_1 + \color{red}\epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable  
$\beta$ - model parameters  
$\beta_0$ - intercept  
$\beta_1$ - slope  
$\epsilon$ - random error


`@script`
> And lastly the random error term epsilon. The error term measures how much of the variation in the response variable is not explained by the explanatory variable.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "8ce5763dc6"
disable_transition: true
```

`@part1`
**Simple linear model** 

$\normalsize{y = \beta_0 + \beta_1x_1 + \epsilon}$ 

&nbsp;

**General linear model**  
(classical GLM)  

$\normalsize{y = \beta_0 + \beta_1x_1 + ... + \beta_px_p + \epsilon}$


`@part2`
$y$ - response variable
$x$ - explanatory variable  
$\beta$ - model parameters  
$\beta_0$ - intercept  
$\beta_1$ - slope  
$\epsilon$ - random error


`@script`
> Extending the simple linear model to introduce more than one explanatory variable we call general linear model or the classical GLM for short.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "bc24f029fc"
```

`@part1`
Want to model

$\normalsize{E[y|X] = \mu}$

$\normalsize{ \mu = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$


`@part2`



`@script`
> One of the main goals of regression is to predict Y from the explanatory variables using the relationship as the following. Or in other words we model the information of how much the response variable y changes on average for a unit increase in the explanatory variable x.

> Note that we write the expectation as y conditional on x due to the fact that the inferences about the relationship of y and x assume knowledge of x.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "b9da57178e"
disable_transition: true
```

`@part1`
Want to model

$\normalsize{E[\color{blue}{y|X}] = \mu}$

$\normalsize{ \mu = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$


`@part2`



`@script`
> Note that we write the expectation as y conditional on x due to the fact that the inferences about the relationship of y and x assume knowledge of x.


---
## Review of Linear Models

```yaml
type: "TwoColumns"
key: "cbed00212c"
```

`@part1`
Want to model

$\normalsize{E[y|X] = \mu}$ 

$\normalsize{ \mu = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$


`@part2`
**Assumptions** 
- linear in parameters {{2}}
- errors: independent and normally distributed $N(0,\sigma^2)$ {{3}}
- constant variance (homoscedasticity)  
$Var(y_i) = \sigma^2$ {{4}}


`@script`
> Linear models have the following assumptions:

> the model is linear in parameters. 

> the errors are independent, normally distributed with mean zero and variance sigma squared

> constant variance, which means that the variance around the regression line is constant for all values of x


---
## Review of Linear Models

```yaml
type: "FullSlide"
key: "115cd470fb"
```

`@part1`
**WHAT IF ... ?**  
- our measurements of the response are not continuous, but for example binary or count {{1}}
- the variance of $y$ depends on the mean {{2}}


`@script`
> Considering all that several what if questions arise,

> first and foremost being, what if my data is not continuous. 

> or what if we donâ€™t have constant variance, but rather the variance depends on the mean.  
To illustrate the problem let's consider an example.


---
## Motivating Example - Data

```yaml
type: "FullSlide"
key: "55ff39cc92"
```

`@part1`
- Contamination of ground water with arsenic in Bangladesh 
- model the decision on switching wells 

|Variables|Name|Description|
|:-|-|:-|
|Response|`switch`|1 if change of water source occurred; 0 otherwise|
|Explanatory|`arsenic`| The level of arsenic contamination in the well|
|Explanatory|`distance`| Distance to the closest known safe well|
|Explanatory|`education`|Years of education of the head of the household|


`@script`
> For this example we will use a dataset on the contamination of ground water with arsenic in Bangladesh. According to the world health organization, this is the largest poisoning of a population in history, with millions of people exposed.

> using this dataset we want to model the decision on switching the current well that the household uses.

> The dataset consists of a response variable switch and three explanatory variables arsenic, distance and education.


---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "c497ddb85d"
```

`@part1`
```python
formula = 'switch ~ arsenic' 
``` {{1}}
```python
family = sm.families.Gaussian() 
``` {{3}}
```py
smf.glm(formula = formula, data = wells, family = family).fit()
``` {{4}}

![](http://assets.datacamp.com/production/repositories/3864/datasets/3ef6d3d112099677e1ca9fb09c9961cf2b1de7c9/Ch1_L1_Scatter_plot_arsenic.png)
{{2}}


`@script`
> First we fit a linear model where switch is predicted by arsenic. 

> Keep in mind that our response variable is binary and not continuous, as can be seen from the figure. the linear model predicts the mean of y , which in this case is the probability that y is equal to 1. We can interpret it as in linear regression, i.e. for a one unit increase in X, the parameter beta will tells us by how much the probability that y=1 increases.

> We define a Gaussian distribution for the response.

> and use the GLM function from the statsmodels library to fit the model. We specify the functional form of the model, distribution of the data and our dataset.


---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "551e61472a"
disable_transition: true
```

`@part1`
```python
formula = 'switch ~ arsenic' 

family = sm.families.Gaussian() 

smf.glm(formula = formula, data = wells, family = family).fit()
```
![](http://assets.datacamp.com/production/repositories/3864/datasets/de114467b003fea04daa83aae23dce73b4d1bade/Ch1_L1_Scatter_plot_LM_arsenic_basic.png)


`@script`
> We obtained the linear fit denoted in red line.


---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "9a7702ce6a"
disable_transition: true
```

`@part1`
```python
formula = 'switch ~ arsenic'

family = sm.families.Gaussian()

smf.glm(formula = formula, data = wells, family = family).fit() 
``` 

![](http://assets.datacamp.com/production/repositories/3864/datasets/4ee031be36efd84878e82ca015eaf4d2160217ce/Ch1_L1_Scatter_plot_LM_arsenic.png)


`@script`
> Taking arsenic level to be equal to 8 we immidiately see this is structurally wrong, since we get value greter than 1.


---
## Motivating example - the LM model

```yaml
type: "FullSlide"
key: "4cc5f907b4"
```

`@part1`
Conclusion: 
- estimated probability outside the (0,1) range
- distribution  of $y$ is highly skewed when $p$ is close to 0 or 1 - not Gaussian
- variance of $y$ depends on the mean value


`@script`
> To conclude, there is a clear violation of the estiamted probability values
> Distribution function is not gaussian, and 
> the variance of Y depends on the mean value.


---
## Motivating example - the GLM 

```yaml
type: "FullSlide"
key: "387f3c5591"
```

`@part1`
```python
formula = 'switch ~ arsenic' 
``` {{1}}
```python
family = sm.families.Binomial() 
``` {{2}}
```py
smf.glm(formula = formula, data = wells, family = family).fit()
``` {{3}}

![](http://assets.datacamp.com/production/repositories/3864/datasets/3ef6d3d112099677e1ca9fb09c9961cf2b1de7c9/Ch1_L1_Scatter_plot_arsenic.png)
{{1}}


`@script`
> Using the same data and model formula, let's change the distribution to Binomial


---
## Motivating example - the GLM

```yaml
type: "FullSlide"
key: "33c3df5544"
disable_transition: true
```

`@part1`
```python
formula = 'switch ~ arsenic' 

family = sm.families.Binomial() 

smf.glm(formula = formula, data = wells, family = family).fit()
``` 

![](http://assets.datacamp.com/production/repositories/3864/datasets/846288f2ee857a9bdf6681e001d701ce6981487b/Ch1_L1_Scatter_plot_GLM_arsenic.png)


`@script`
> The fit looks much better now
> We can conclude that we have the correct model formulation, where
> the estimated probabilities are bounded by 0 and 1


---
## Motivating example - Summarize

```yaml
type: "FullSlide"
key: "392587dd68"
```

`@part1`
- $\color{red}{\text{NOK}}$ - Ignoring the distribution of the response {{1}}
- $\color{green}{\text{OK}}$ - Specifying the Binomial distribution for the binary response {{2}}
- Introducing the function $g$ (*link function*) {{4}}

$$\normalsize{E[y]= \beta_0 + \beta_1x_1 + ... + \beta_px_p}$$ {{4}}

$$\normalsize{\color{red}{g}(E[y]) = \beta_0 + \beta_1x_1 + ... + \beta_px_p}$$
{{4}}


`@script`
> Let's recap and see what changed?

> Igoring the distribution of the response and fitting a linear model lead to structurally wrong results

> To correct for this we specified the distribution for the response variable to be Binomial given the binary response.
But is this enough? Not quite. 

> We still need to transform the mean of Y so that it is linearly related to X. It can be written as follows. The details of which we will discuss in the next video.


---
## Let's practice!

```yaml
type: "FinalSlide"
key: "72f692d178"
```

`@script`
Now it is time for some practice problems.

